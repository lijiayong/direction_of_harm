{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIfVwemPCtWHcy2KKH1aCG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for scraping Reddit. I followed [this video](https://www.youtube.com/watch?v=gIZJQmX-55U) for getting the Reddit ID and Reddit secret."
      ],
      "metadata": {
        "id": "EPPkq1KC7BTE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LZYp-3KdzxwP"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "REDDIT_ID = userdata.get('REDDIT_ID')\n",
        "REDDIT_SECRET = userdata.get('REDDIT_SECRET')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_path = '/content/drive/MyDrive'"
      ],
      "metadata": {
        "id": "D0ce632N05DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "id": "L5rQvsV-1ru9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np, pandas as pd\n",
        "import praw\n",
        "from praw.models import MoreComments"
      ],
      "metadata": {
        "id": "aFhcTkB-2EKU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process post\n",
        "We split a full post into paragraphs, and then split each paragraph into texts that are at most 10 sentences long. We keep the title, and only keep the first 99 such splits of a post."
      ],
      "metadata": {
        "id": "LpZVc8BYZ001"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process(id, title, text, max_text_split=99, max_sentences=10):\n",
        "    result_list = [f'{id}_00\\t{title.strip()}\\n']\n",
        "    paragraphs = text.split('\\n')\n",
        "    paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip() != '']\n",
        "    split_id = 1\n",
        "    for i, paragraph in enumerate(paragraphs):\n",
        "        if split_id > max_text_split:\n",
        "            break\n",
        "        sentences = paragraph.split('.')\n",
        "        sentences = [sentence.strip() for sentence in sentences if sentence.strip() != '']\n",
        "        for j in range(0, len(sentences), max_sentences):\n",
        "            if split_id > max_text_split:\n",
        "                break\n",
        "            split_id_str = str(split_id).zfill(2)\n",
        "            split_text = '. '.join(sentences[j: j+max_sentences]) + '\\n'\n",
        "            result_list.append(f'{id}_{split_id_str}\\t{split_text}')\n",
        "            split_id += 1\n",
        "    return result_list"
      ],
      "metadata": {
        "id": "oO4j6G0ee4ZL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split a comment into paragraphs, and then split each paragraph into texts that are at most 10 sentences long. Comments have no title, we only keep the first 100 such splits of a comment."
      ],
      "metadata": {
        "id": "9P9OmNyTtyAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_comment(id, body, max_text_split=100, max_sentences=10):\n",
        "    result_list = []\n",
        "    if body.startswith(\"Thank you for posting in r/abusiverelationships.\"):\n",
        "        return result_list\n",
        "    paragraphs = body.split('\\n')\n",
        "    paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip() != '']\n",
        "    split_id = 0\n",
        "    for i, paragraph in enumerate(paragraphs):\n",
        "        if split_id > max_text_split:\n",
        "            break\n",
        "        sentences = paragraph.split('.')\n",
        "        sentences = [sentence.strip() for sentence in sentences if sentence.strip() != '']\n",
        "        for j in range(0, len(sentences), max_sentences):\n",
        "            if split_id > max_text_split:\n",
        "                break\n",
        "            split_id_str = str(split_id).zfill(2)\n",
        "            split_text = '. '.join(sentences[j: j+max_sentences]) + '\\n'\n",
        "            result_list.append(f'{id}_{split_id_str}\\t{split_text}')\n",
        "            split_id += 1\n",
        "    return result_list"
      ],
      "metadata": {
        "id": "P-sVbt5IZjO2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(client_id=REDDIT_ID, client_secret=REDDIT_SECRET, user_agent='Reddit Scraper', check_for_async=False)"
      ],
      "metadata": {
        "id": "47D4znOwNUWL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape(subreddit_name, version, num_of_posts=900):\n",
        "    print(f'Subreddit: {subreddit_name}')\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    count = 1\n",
        "    with open(f'{save_path}/subreddit_{subreddit_name}_{num_of_posts}_{version}.tsv', 'w') as f:\n",
        "        f.write(f'id\\ttext\\n')\n",
        "        for submission in subreddit.new(limit=num_of_posts):\n",
        "            if count == 1:\n",
        "                utc = submission.created_utc\n",
        "                print(f'\\tTime of last post: {datetime.fromtimestamp(utc):%Y-%m-%d %H:%M:%S}')\n",
        "            if count == num_of_posts:\n",
        "                utc = submission.created_utc\n",
        "                print(f'\\tTime of first post: {datetime.fromtimestamp(utc):%Y-%m-%d %H:%M:%S}')\n",
        "            for result in process(submission.id, submission.title, submission.selftext):\n",
        "                f.write(result)\n",
        "            count += 1"
      ],
      "metadata": {
        "id": "jQZ4__qhclP3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_comments(subreddit_name, version, num_of_posts=600):\n",
        "    print(f'Subreddit: {subreddit_name}')\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    count = 1\n",
        "    with open(f'{save_path}/subreddit_{subreddit_name}_{num_of_posts}_{version}_comments.tsv', 'w') as f:\n",
        "        f.write(f'id\\ttext\\n')\n",
        "        for submission in subreddit.new(limit=num_of_posts):\n",
        "            if count == 1:\n",
        "                utc = submission.created_utc\n",
        "                print(f'\\tTime of last post: {datetime.fromtimestamp(utc):%Y-%m-%d %H:%M:%S}')\n",
        "            if count == num_of_posts:\n",
        "                utc = submission.created_utc\n",
        "                print(f'\\tTime of first post: {datetime.fromtimestamp(utc):%Y-%m-%d %H:%M:%S}')\n",
        "            for comment in submission.comments.list():\n",
        "                if isinstance(comment, MoreComments):\n",
        "                    continue\n",
        "                for result in process_comment(comment.id, comment.body):\n",
        "                    f.write(result)\n",
        "            count += 1"
      ],
      "metadata": {
        "id": "3oCz6CEkYmN0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape r/SuicideWatch and r/abusiverelationships"
      ],
      "metadata": {
        "id": "aUMRf2l2awX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scrape('SuicideWatch', 'v2')\n",
        "scrape('abusiverelationships', 'v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InqviJYpavXQ",
        "outputId": "6ccf6cf4-51b4-4805-af54-10efa59f683b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subreddit: SuicideWatch\n",
            "\tTime of last post: 2024-09-20 12:53:18\n",
            "\tTime of first post: 2024-09-17 21:21:40\n",
            "Subreddit: abusiverelationships\n",
            "\tTime of last post: 2024-09-20 12:12:02\n",
            "\tTime of first post: 2024-08-31 08:01:33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_comments('abusiverelationships', 'v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyohVK9xbWZ3",
        "outputId": "444188f2-30b8-4275-bb7e-955dc147d2fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subreddit: abusiverelationships\n",
            "\tTime of last post: 2024-09-21 09:46:33\n",
            "\tTime of first post: 2024-09-09 00:35:04\n"
          ]
        }
      ]
    }
  ]
}